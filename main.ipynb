{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLP Homework 2 - OCRed text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_fn = \"cleaned.json\"\n",
    "ocred_fn = \"original_ocr.json\"\n",
    "\n",
    "load_dotenv()\n",
    "directory_path = os.environ['DATA_PATH']\n",
    "login(token=os.environ['HF_TOKEN'])\n",
    "genai.configure(api_key=os.environ['GEMINI_KEY']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRaw = {}\n",
    "\n",
    "for name in [ocred_fn, cleaned_fn]:\n",
    "    if name not in os.listdir(directory_path):\n",
    "        print(f\"File [{name}] not Found in directory [{directory_path}]\")\n",
    "\n",
    "    file_path = os.path.join(directory_path, name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        datasetRaw[i.split('.')[0]] = json.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minerva 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='cuda', torch_dtype=torch.float16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
