{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLP Homework 2 - OCRed text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from rouge import Rouge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050\n",
      "Supports float16: True\n",
      "Supports bfloat16: True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Supports float16:\", torch.cuda.is_available())\n",
    "print(\"Supports bfloat16:\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to either set a .env file or put here the keys for gemini and hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fede6\\Desktop\\MNLP-Homework2\n"
     ]
    }
   ],
   "source": [
    "%cd Desktop/MNLP-Homework2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.env' in os.listdir(os.getcwd()):\n",
    "    load_dotenv()\n",
    "    CACHE_SUBPATH = os.environ['CACHE_PATH']\n",
    "    TEXTS_SUBPATH = os.environ['DATA_PATH']\n",
    "    OUTPUT_SUBPATH = os.environ['OUTPUT_PATH']\n",
    "    HF_TOKEN = os.environ['HF_TOKEN']\n",
    "    GEMINI_KEY = os.environ['GEMINI_KEY']\n",
    "    \n",
    "else:\n",
    "    CACHE_SUBPATH = 'cache'\n",
    "    TEXTS_SUBPATH = 'data\\ita'\n",
    "    OUTPUT_SUBPATH = 'cleaned'\n",
    "    HF_TOKEN = ''\n",
    "    GEMINI_KEY = ''\n",
    "\n",
    "os.environ[\"ACCELERATE_USE_TORCH_DEVICE\"] = \"true\"\n",
    "\n",
    "TEXTS_PATH = os.path.join(os.getcwd(), TEXTS_SUBPATH)\n",
    "OUTPUT_PATH =os.path.join(os.getcwd(), OUTPUT_SUBPATH)\n",
    "CACHE_PATH = os.path.join(os.getcwd(), CACHE_SUBPATH)\n",
    "\n",
    "os.makedirs(TEXTS_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(CACHE_PATH, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = CACHE_PATH\n",
    "\n",
    "FILE_LIST = [\"original_ocr.json\", \"cleaned.json\"]\n",
    "OUTPUT_PREFIX = \"Pizza_Language_&_Mandolino-hw2_ocr\"\n",
    "\n",
    "assert HF_TOKEN != '', \"No key for hugging face\"\n",
    "login(token=HF_TOKEN)\n",
    "assert GEMINI_KEY != '', \"No key for gemini\"\n",
    "genai.configure(api_key=GEMINI_KEY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRaw = {}\n",
    "\n",
    "for name in FILE_LIST:\n",
    "    if name not in os.listdir(TEXTS_PATH):\n",
    "        print(f\"File [{name}] not Found in directory [{TEXTS_PATH}]\")\n",
    "\n",
    "    file_path = os.path.join(TEXTS_PATH, name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        datasetRaw[name.split('.')[0]] = json.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_split_paragraphs(text:str)->list[str]:\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "    text = re.sub(r'\\.{2,}', lambda m: f\"<DOTS{len(m.group(0))}>\", text)\n",
    "\n",
    "    fragments = re.split(r'(?<!\\.)\\.(?!\\.)(?=\\s|$)', text)\n",
    "\n",
    "    result = []\n",
    "    for frag in fragments:\n",
    "        frag = frag.strip()\n",
    "        if not frag:\n",
    "            continue\n",
    "        frag = re.sub(r'<DOTS(\\d+)>', lambda m: '.' * int(m.group(1)), frag)\n",
    "        result.append(frag + \". \")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hyphenated_words(text:str)->str:\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. \n",
      "Como andò che Maestro Ciliegia, Megnamc trovò un pezzo di legno che piangeva e rideva come un bambino. \n",
      "— C'era una volta.... — Un re! -diranno subito i miei piccoli lettori. \n",
      "— Ko, ragazzi, avete sbagliato. \n",
      "C'era una volta un pezzo di legno. \n",
      "Kon era un legno di lusso, ma un semplice pezzo da catasta, di quelli che d' inverno si mettono nelle stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze. \n",
      "Non so come andasse, ma il fatto gli è che un bel giorno' questo pezzo di legno capitò nella bottega di un vecchio falegname, il quale aveva nome mastr' Antonio, se non che tutti lo chiamavano maestro Ciliegia, per via della punta del sentì nna vocina sottile sottile. \n",
      "SUO naso, che era sempre lustra e paonazza, come una ciliegia matura. \n",
      "Appena maestro Ciliegia ebbe visto quel pezzo di legno, si rallegrò tutto; e dandosi una fregatina di mani per la contentezza, borbottò a mezza voce: — Questo legno è capitato a tempo; voglio servirmene per fare una gamba di tavolino. \n",
      "— Detto fatto, prese subito l'ascia arrotata per cominciare a levargli la scorza e a digrossarlo; ma quando fu lì per lasciare andare la prima asciata, rimase col braccio sospeso in aria, perchè sentì una vocina sottile sottile, che disse raccomandandosi: — Non mi picchiar tanto forte! — Figuratevi come rimase quel buon vecchio di maestro Ciliegia! Girò gli occhi smarriti intorno alla stanza per vedere di dove mai poteva essere uscita quella vocina, e non vide nessuno ! Guardò sotto il banco, e nessuno : guardò dentro un armadio che stava sempre chiuso, e nessuno; guardò nel corbèllo dei trucioli e della segatura, e nessuno; aprì l'uscio di bottega per dare un'occhiata anche sulla strada, e nessuno. \n",
      "O dunque?... — Ho capito; — disse allora ridendo e gTattandosi la parrucca — si vede che quella vocina me la son figurata io. \n",
      "Eimettiamoci a lavorare. \n",
      "— E ripresa l' ascia in mano, tirò giù un solennissimo colpo sul pezzo di legno. \n",
      "— Ohi! tu m'hai fatto male! — gridò rammaricandosi la solita vocina. \n",
      "Questa volta maestro Ciliegia restò di stucco, cogli occhi fuori del capo per la paura, colla bocca spalancati e colla lingna giù ciondoloni fino al mento, come nn mascherone da fontana. \n",
      "Appena riebbe l'uso della parola, cominciò a dire tremando e balbettando dallo spavento: — Ma di dove sarà uscita questa vocina che ha detto ohi ?... Eppure qui non e' è anima viva. \n",
      "Ohe sia per caso questo pezzo di legno che abbia imparato a piangere e a lamentarsi come un bambino? Io non lo posso credere. \n",
      "Questo legno eccolo qui; è un pezzo di legno da caminetto, come tutti gli altri, e a buttarlo sul fuoco, c'è da far bollire una pentola di fagioli.... O dunque? Ohe ci sia nascosto dentro qualcuno! Se c'è nascosto qualcuno, tanto peggio per lui. \n",
      "Ora l'accomodo io! — E così dicendo, agguantò con tutt' e due le mani quel povero pezzo di legno, e si pose a sbatacchiarlo senza carità contro le pareti della stanza. \n",
      "Poi si messe in ascolto, per sentire se c'era qualche vocina che si lamentasse. \n",
      "Aspettò due minuti, e nulla; cinque minuti, e nulla; dieci minuti, e nulla! — - Ho capito — disse allora sforzandosi di ridere e arruffandosi la parrucca — si vede che quella vocina che ha detto olii, me la son figurata io! Eimettiamoci a lavorare. \n",
      "— E perchè gli era entrato addosso una gran paura, si provò a canterellare per farsi un po' di coraggio. \n",
      "Intanto, i^osata da una parte l'ascia, prese in mano la pialla, per piallare e tirare a pulimento il pezzo di legno ; ma nel mentre che lo piallava in su e in giù, sentì la solita vocina che gli disse ridendo : — Smetti! tu mi fai il pizzicorino sul corpo! — Questa volta il povero maestro Ciliegia cadde giù come fulminato. \n",
      "Quando riaprì gli occhi, si trovò seduto per terra. \n",
      "Il suo viso pareva trasfigurito, e perfino la punta del naso, di paonazza come era quasi sempre, gli era diventata turchina dalla gran x>ai^i'?^' pyyyyy^y^'^wggfy; ti;;::;i;r. \n"
     ]
    }
   ],
   "source": [
    "sample = datasetRaw['original_ocr']['1']\n",
    "sentences = smart_split_paragraphs(sample)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = merge_hyphenated_words(sentences[i])\n",
    "\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    print(f\"{s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(texts: list[str], model_obj: dict)-> list[dict]:\n",
    "    results = []\n",
    "    for text in texts:        \n",
    "        prompt = model_obj['prompt_gen'](text)\n",
    "        \n",
    "        output = model_obj['generator'](\n",
    "            prompt,\n",
    "            max_new_tokens=len(model_obj['tokenizer'].encode(text)) - 2,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            num_beams=4,\n",
    "            repetition_penalty=1.1,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        cleaned = output.split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "        print(f\"Input:  {text}\")\n",
    "        print(f\"Output: {cleaned}\\n\")\n",
    "        \n",
    "        results.append({\"input\": text, \"output\": cleaned})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiment(model_obj:dict) -> None:\n",
    "    print(model_obj['name'])\n",
    "    for chapter, sample in datasetRaw['original_ocr'].items():\n",
    "        print(f\"Chapter {chapter}\")\n",
    "        sentences = smart_split_paragraphs(sample)\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = merge_hyphenated_words(sentences[i])\n",
    "        corrected_data = correct_text(sentences, model_obj)\n",
    "\n",
    "        with open(os.path.join(OUTPUT_PATH,model_obj['name'], f\"Chapter_{chapter}.json\"), \"w+\", encoding=\"utf-8\") as file:\n",
    "            json.dump(corrected_data, file, ensure_ascii=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_output(model_name:str, prefix:str=OUTPUT_PREFIX)->None:\n",
    "    cleaned_txt = {}\n",
    "    dest_path = os.path.join(OUTPUT_PATH, model_name)\n",
    "    for i, file_name in enumerate(os.listdir(dest_path)):\n",
    "        with open(os.path.join(dest_path,file_name), 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            text = ' '.join(item['output'] for item in data)\n",
    "            cleaned_txt[str(i+1)] = text\n",
    "            file.close()\n",
    "\n",
    "    fname = f\"{prefix}-{model_name}.json\"\n",
    "    with open(os.path.join(OUTPUT_PATH, fname), 'w', encoding='utf-8') as f_out:\n",
    "        json.dump(cleaned_txt, f_out, ensure_ascii=False, indent=2)\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minerva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c076dbbe9c438aa6c2d0f2b6d8d14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "MINERVA_VERSION = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
    "minerva_tok = AutoTokenizer.from_pretrained(MINERVA_VERSION, trust_remote_code=True)\n",
    "minerva_model = AutoModelForCausalLM.from_pretrained(MINERVA_VERSION, \n",
    "                                            trust_remote_code=True, \n",
    "                                            device_map=device, \n",
    "                                            torch_dtype=torch.float16\n",
    "                                            )\n",
    "\n",
    "minerva_gen = pipeline(\"text-generation\", \n",
    "                        model=minerva_model, \n",
    "                        tokenizer=minerva_tok, \n",
    "                        pad_token_id=minerva_tok.eos_token_id\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "minerva_obj = {\n",
    "    \"model\":minerva_model,\n",
    "    \"generator\": minerva_gen,\n",
    "    \"tokenizer\":minerva_tok,\n",
    "    \"name\":\"Minerva\",\n",
    "    \"prompt_gen\": lambda x: (\n",
    "            f\"Sei un generatore di eBooks. \"\n",
    "            f\"Correggi TUTTI gli errori OCR nel testo che ti verrà fornito. \"\n",
    "            f\"Non aggiungere, togliere o cambiare parole. \"\n",
    "            f\"Non modificare la punteggiatura esistente. \"\n",
    "            f\"Non interpretare, riassumere o riscrivere il testo. \"\n",
    "            f\"Mantieni esattamente lo stesso numero di parole e l'ordine delle parole. \"\n",
    "            f\"Il testo corretto deve conservare il significato originale inalterato. \"\n",
    "            f\"La tua unica funzione è la correzione di errori OCR: lettere errate, parole spezzate, ed errori di capitalizzazioni. \"\n",
    "            f\"Rendi maiuscole le iniziali di frase e i nomi propri. Rendi minuscole le parole che non sono nomi propri o inizi di frase ma sono in maiuscolo. \"\n",
    "            f\"Di seguito sono forniti esempi di input OCR e il corrispondente output corretto. Apprendi da questi esempi per svolgere il tuo compito.\\n\"\n",
    "            f\"Esempi di correzione OCR (inclusa capitalizzazione):\\n\"\n",
    "            f\"Input:  'qvesto è un testo. sembra ocr.'\\n\"\n",
    "            f\"Output: 'Questo è un testo. Sembra OCR.'\\n\"\n",
    "            f\"Input:  'l'aquila vola alta. il cieto è blù.'\\n\"\n",
    "            f\"Output: 'L'aquila vola alta. Il cielo è blu.'\\n\"\n",
    "            f\"Input: 'la matita è rota. c'è del tem po.'\\n\"\n",
    "            f\"Output: 'La matita è rotta. C'è del tempo.'\\n\"\n",
    "            f\"Input: 'i1 libro è su1 tavolo. LA STAMPA è chiara.'\\n\"\n",
    "            f\"Output: 'Il libro è sul tavolo. La stampa è chiara.'\\n\"\n",
    "            f\"Input: 'abbiam o un gTande paeco. la vitta è bella.'\\n\"\n",
    "            f\"Output: 'Abbiamo un grande pacco. La vita è bella.'\\n\"\n",
    "            f\"Testo OCR: {x} \"\n",
    "            f\"Testo corretto:\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minerva\n",
      "Chapter 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mexecute_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminerva_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m reformat_output(minerva_obj[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mexecute_experiment\u001b[39m\u001b[34m(model_obj)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n\u001b[32m      7\u001b[39m     sentences[i] = merge_hyphenated_words(sentences[i])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m corrected_data = \u001b[43mcorrect_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(OUTPUT_PATH,model_obj[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChapter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchapter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     11\u001b[39m     json.dump(corrected_data, file, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcorrect_text\u001b[39m\u001b[34m(texts, model_obj)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:        \n\u001b[32m      4\u001b[39m     prompt = model_obj[\u001b[33m'\u001b[39m\u001b[33mprompt_gen\u001b[39m\u001b[33m'\u001b[39m](text)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     output = \u001b[43mmodel_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgenerator\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokenizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m     cleaned = output.split(\u001b[33m\"\u001b[39m\u001b[33mTesto corretto:\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].strip().split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:385\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    383\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    388\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2484\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n\u001b[32m   2483\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2484\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2494\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2495\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2496\u001b[39m         batch_size=batch_size,\n\u001b[32m   2497\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2503\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2504\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:3907\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3904\u001b[39m model_outputs = \u001b[38;5;28mself\u001b[39m(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3906\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3907\u001b[39m model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3911\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3912\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[32m   3913\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:919\u001b[39m, in \u001b[36mGenerationMixin._update_model_kwargs_for_generation\u001b[39m\u001b[34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    918\u001b[39m     past_positions = model_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m     new_positions = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to(past_positions.device)\n\u001b[32m    922\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat((past_positions, new_positions))\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "execute_experiment(minerva_obj)\n",
    "reformat_output(minerva_obj['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fede6\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971c9327b75646b4b665f2b3a4a1ee52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "LLAMA_VERSION = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "llama_tok = AutoTokenizer.from_pretrained(LLAMA_VERSION)\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_VERSION,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "llama_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tok,\n",
    "    pad_token_id=llama_tok.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_obj = {\n",
    "    \"model\":llama_model,\n",
    "    \"generator\":llama_gen,\n",
    "    \"tokenizer\":llama_tok,\n",
    "    \"name\":\"Llama\",\n",
    "    \"prompt_gen\": lambda x: (\n",
    "            f\"Sei un Grillo Parlante! \"\n",
    "            f\"Correggi TUTTI gli errori OCR nel testo che ti verrà fornito. \"\n",
    "            f\"Non aggiungere, togliere o cambiare parole. \"\n",
    "            f\"Non modificare la punteggiatura esistente. \"\n",
    "            f\"Non interpretare, riassumere o riscrivere il testo. \"\n",
    "            f\"Mantieni esattamente lo stesso numero di parole e l'ordine delle parole. \"\n",
    "            f\"Il testo corretto deve conservare il significato originale inalterato. \"\n",
    "            f\"La tua unica funzione è la correzione di errori OCR: lettere errate, parole spezzate, ed errori di capitalizzazioni. \"\n",
    "            f\"Rendi maiuscole le iniziali di frase e i nomi propri. Rendi minuscole le parole che non sono nomi propri o inizi di frase ma sono in maiuscolo. \"\n",
    "            f\"Di seguito sono forniti esempi di input OCR e il corrispondente output corretto. Apprendi da questi esempi per svolgere il tuo compito.\\n\"\n",
    "            f\"Esempi di correzione OCR (inclusa capitalizzazione):\\n\"\n",
    "            f\"Input: 'qvesto è un testo. sembra ocr.'\\n\"\n",
    "            f\"Output: 'Questo è un testo. Sembra OCR.'\\n\"\n",
    "            f\"Input: 'l'aquila vola alta. il cieto è blù.'\\n\"\n",
    "            f\"Output: 'L'aquila vola alta. Il cielo è blu.'\\n\"\n",
    "            f\"Input: 'la matita è rota. c'è del tem po.'\\n\"\n",
    "            f\"Output: 'La matita è rotta. C'è del tempo.'\\n\"\n",
    "            f\"Input: 'i1 libro è su1 tavolo. LA STAMPA è chiara.'\\n\"\n",
    "            f\"Output: 'Il libro è sul tavolo. La stampa è chiara.'\\n\"\n",
    "            f\"Input: 'abbiam o un gTande paeco. la vitta è bella.'\\n\"\n",
    "            f\"Output: 'Abbiamo un grande pacco. La vita è bella.'\\n\"\n",
    "            f\"Input: 'rn'\\n\"\n",
    "            f\"Output: 'm'\\n\"\n",
    "            f\"Input: 'Ko'\\n\"\n",
    "            f\"Output: 'No'\\n\"\n",
    "            f\"Input: '1ibro'\\n\"  \n",
    "            f\"Output: 'libro'\\n\"\n",
    "            f\"Input: '0cchio'\\n\" \n",
    "            f\"Output: 'occhio'\\n\"\n",
    "            f\"Input: 'cl'\\n\" \n",
    "            f\"Output: 'd'\\n\"\n",
    "            f\"Input: 'e, '\\n\"\n",
    "            f\"Output: 'e '\\n\"\n",
    "            f\"Testo OCR: {x} \"\n",
    "            f\"Testo corretto:\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama\n",
      "Chapter 1\n",
      "Input:  I. \n",
      "Output: I.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mexecute_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m reformat_output(llama_obj[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mexecute_experiment\u001b[39m\u001b[34m(model_obj)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentences)):\n\u001b[32m      7\u001b[39m     sentences[i] = merge_hyphenated_words(sentences[i])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m corrected_data = \u001b[43mcorrect_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(OUTPUT_PATH,model_obj[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChapter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchapter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     11\u001b[39m     json.dump(corrected_data, file, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcorrect_text\u001b[39m\u001b[34m(texts, model_obj)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:        \n\u001b[32m      4\u001b[39m     prompt = model_obj[\u001b[33m'\u001b[39m\u001b[33mprompt_gen\u001b[39m\u001b[33m'\u001b[39m](text)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     output = \u001b[43mmodel_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgenerator\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokenizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m     cleaned = output.split(\u001b[33m\"\u001b[39m\u001b[33mTesto corretto:\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].strip().split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:385\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    383\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    388\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:2484\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n\u001b[32m   2483\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2484\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2494\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2495\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2496\u001b[39m         batch_size=batch_size,\n\u001b[32m   2497\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2503\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2504\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:3904\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3901\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3902\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m-> \u001b[39m\u001b[32m3904\u001b[39m model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3906\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3907\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3908\u001b[39m     model_outputs,\n\u001b[32m   3909\u001b[39m     model_kwargs,\n\u001b[32m   3910\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3911\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:318\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:253\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    252\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    254\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    256\u001b[39m cos, sin = position_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\hooks.py:171\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    173\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\hooks.py:361\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    354\u001b[39m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    355\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m value.data_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map\n\u001b[32m    357\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tied_params_map[value.data_ptr()]\n\u001b[32m    358\u001b[39m         ):\n\u001b[32m    359\u001b[39m             \u001b[38;5;28mself\u001b[39m.tied_pointers_to_remove.add((value.data_ptr(), \u001b[38;5;28mself\u001b[39m.execution_device))\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m.execution_device), send_to_device(\n\u001b[32m    371\u001b[39m     kwargs, \u001b[38;5;28mself\u001b[39m.execution_device, skip_keys=\u001b[38;5;28mself\u001b[39m.skip_keys\n\u001b[32m    372\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\utils\\modeling.py:337\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[39m\n\u001b[32m    335\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "execute_experiment(llama_obj)\n",
    "reformat_output(llama_obj['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_VERSION = \"gemini-1.5-flash\"\n",
    "\n",
    "def segment_wrapper(segment_ocred, segment_clean):\n",
    "    return f\"\"\"\n",
    "        ### Task: Il primo testo è una versione del secondo estratto con oc-Red, ed è stata processata per ridurre gli errori, valuta con un punteggio tra 1 e 100 tenendo conto di correttezza, comprensibilità e somiglianza\n",
    "        ### Testo da valutare: {segment_ocred}.\n",
    "        ### Testo di confronto: {segment_clean}.\n",
    "        ### Requisiti:\n",
    "            - Scrivi il risultato in formato <criterio>:<punteggio>.\n",
    "            - Non usare altri numeri interi o fai altri commenti\n",
    "        ### Risultato:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'Llama': [{'rouge-1': {'r': 0.912568306010929, 'p': 0.9027027027027027, 'f': 0.9076086906523217}, 'rouge-2': {'r': 0.867862969004894, 'p': 0.8594507269789984, 'f': 0.8636363586364824}, 'rouge-l': {'r': 0.912568306010929, 'p': 0.9027027027027027, 'f': 0.9076086906523217}}, {'rouge-1': {'r': 0.901685393258427, 'p': 0.8916666666666667, 'f': 0.8966480396928935}, 'rouge-2': {'r': 0.8731466227347611, 'p': 0.8520900321543409, 'f': 0.8624898241301184}, 'rouge-l': {'r': 0.901685393258427, 'p': 0.8916666666666667, 'f': 0.8966480396928935}}, {'rouge-1': {'r': 0.9134438305709024, 'p': 0.9001814882032668, 'f': 0.9067641631903953}, 'rouge-2': {'r': 0.8772455089820359, 'p': 0.8694362017804155, 'f': 0.8733233929136619}, 'rouge-l': {'r': 0.9134438305709024, 'p': 0.9001814882032668, 'f': 0.9067641631903953}}, {'rouge-1': {'r': 0.8983516483516484, 'p': 0.8582677165354331, 'f': 0.8778523439958921}, 'rouge-2': {'r': 0.8470394736842105, 'p': 0.8253205128205128, 'f': 0.8360389560398043}, 'rouge-l': {'r': 0.8983516483516484, 'p': 0.8582677165354331, 'f': 0.8778523439958921}}, {'rouge-1': {'r': 0.8910614525139665, 'p': 0.8552278820375335, 'f': 0.8727770127859631}, 'rouge-2': {'r': 0.8432708688245315, 'p': 0.8236272878535774, 'f': 0.8333333283340278}, 'rouge-l': {'r': 0.8910614525139665, 'p': 0.8552278820375335, 'f': 0.8727770127859631}}, {'rouge-1': {'r': 0.8984375, 'p': 0.8812260536398467, 'f': 0.8897485443234852}, 'rouge-2': {'r': 0.8574938574938575, 'p': 0.8409638554216867, 'f': 0.8491484134919579}, 'rouge-l': {'r': 0.8984375, 'p': 0.8812260536398467, 'f': 0.8897485443234852}}, {'rouge-1': {'r': 0.897119341563786, 'p': 0.9008264462809917, 'f': 0.8989690671649697}, 'rouge-2': {'r': 0.8623103850641773, 'p': 0.8513824884792627, 'f': 0.856811589203102}, 'rouge-l': {'r': 0.897119341563786, 'p': 0.9008264462809917, 'f': 0.8989690671649697}}], 'Minerva': [{'rouge-1': {'r': 0.9043715846994536, 'p': 0.8850267379679144, 'f': 0.894594589595179}, 'rouge-2': {'r': 0.8613376835236541, 'p': 0.8502415458937198, 'f': 0.8557536416776819}, 'rouge-l': {'r': 0.9043715846994536, 'p': 0.8850267379679144, 'f': 0.894594589595179}}, {'rouge-1': {'r': 0.9129213483146067, 'p': 0.8928571428571429, 'f': 0.9027777727783951}, 'rouge-2': {'r': 0.8830313014827018, 'p': 0.8603531300160514, 'f': 0.8715447104480006}, 'rouge-l': {'r': 0.9129213483146067, 'p': 0.8928571428571429, 'f': 0.9027777727783951}}, {'rouge-1': {'r': 0.9189686924493554, 'p': 0.8974820143884892, 'f': 0.9080982661562957}, 'rouge-2': {'r': 0.8762475049900199, 'p': 0.8675889328063241, 'f': 0.8718967179395474}, 'rouge-l': {'r': 0.9189686924493554, 'p': 0.8974820143884892, 'f': 0.9080982661562957}}, {'rouge-1': {'r': 0.8956043956043956, 'p': 0.844559585492228, 'f': 0.8693333283376357}, 'rouge-2': {'r': 0.837171052631579, 'p': 0.8118022328548644, 'f': 0.824291492976892}, 'rouge-l': {'r': 0.8956043956043956, 'p': 0.844559585492228, 'f': 0.8693333283376357}}, {'rouge-1': {'r': 0.8854748603351955, 'p': 0.8614130434782609, 'f': 0.8732782319155492}, 'rouge-2': {'r': 0.8347529812606473, 'p': 0.8207705192629816, 'f': 0.8277026977030596}, 'rouge-l': {'r': 0.8854748603351955, 'p': 0.8614130434782609, 'f': 0.8732782319155492}}, {'rouge-1': {'r': 0.8046875, 'p': 0.8956521739130435, 'f': 0.8477366205287135}, 'rouge-2': {'r': 0.7567567567567568, 'p': 0.8676056338028169, 'f': 0.808398945154518}, 'rouge-l': {'r': 0.8046875, 'p': 0.8956521739130435, 'f': 0.8477366205287135}}, {'rouge-1': {'r': 0.8991769547325102, 'p': 0.8864097363083164, 'f': 0.8927476967367215}, 'rouge-2': {'r': 0.8588098016336057, 'p': 0.8421052631578947, 'f': 0.8503755004886395}, 'rouge-l': {'r': 0.8991769547325102, 'p': 0.8864097363083164, 'f': 0.8927476967367215}}]}\n",
      "{'Llama': [{'rouge-1': {'r': 0.8852459016393442, 'p': 0.8223350253807107, 'f': 0.8526315739541551}, 'rouge-2': {'r': 0.8287112561174551, 'p': 0.7925117004680188, 'f': 0.8102073315256189}, 'rouge-l': {'r': 0.8852459016393442, 'p': 0.8223350253807107, 'f': 0.8526315739541551}}, {'rouge-1': {'r': 0.8792134831460674, 'p': 0.819371727748691, 'f': 0.8482384773910298}, 'rouge-2': {'r': 0.8303130148270181, 'p': 0.7801857585139319, 'f': 0.8044692687478607}, 'rouge-l': {'r': 0.8792134831460674, 'p': 0.819371727748691, 'f': 0.8482384773910298}}, {'rouge-1': {'r': 0.8655616942909761, 'p': 0.793918918918919, 'f': 0.8281938276084381}, 'rouge-2': {'r': 0.8023952095808383, 'p': 0.7635327635327636, 'f': 0.7824817468278971}, 'rouge-l': {'r': 0.8655616942909761, 'p': 0.793918918918919, 'f': 0.8281938276084381}}, {'rouge-1': {'r': 0.8763736263736264, 'p': 0.7799511002444988, 'f': 0.8253557518086654}, 'rouge-2': {'r': 0.7993421052631579, 'p': 0.7431192660550459, 'f': 0.7702060171936479}, 'rouge-l': {'r': 0.8763736263736264, 'p': 0.7799511002444988, 'f': 0.8253557518086654}}, {'rouge-1': {'r': 0.8379888268156425, 'p': 0.7792207792207793, 'f': 0.8075370071196579}, 'rouge-2': {'r': 0.7802385008517888, 'p': 0.7387096774193549, 'f': 0.7589063744569274}, 'rouge-l': {'r': 0.8379888268156425, 'p': 0.7792207792207793, 'f': 0.8075370071196579}}, {'rouge-1': {'r': 0.8671875, 'p': 0.8043478260869565, 'f': 0.8345864611724801}, 'rouge-2': {'r': 0.8304668304668305, 'p': 0.7878787878787878, 'f': 0.8086124351948502}, 'rouge-l': {'r': 0.8671875, 'p': 0.8043478260869565, 'f': 0.8345864611724801}}, {'rouge-1': {'r': 0.8518518518518519, 'p': 0.7976878612716763, 'f': 0.8238805920203164}, 'rouge-2': {'r': 0.7981330221703618, 'p': 0.7533039647577092, 'f': 0.7750708165339196}, 'rouge-l': {'r': 0.8518518518518519, 'p': 0.7976878612716763, 'f': 0.8238805920203164}}], 'Minerva': [{'rouge-1': {'r': 0.8852459016393442, 'p': 0.8223350253807107, 'f': 0.8526315739541551}, 'rouge-2': {'r': 0.8287112561174551, 'p': 0.7925117004680188, 'f': 0.8102073315256189}, 'rouge-l': {'r': 0.8852459016393442, 'p': 0.8223350253807107, 'f': 0.8526315739541551}}, {'rouge-1': {'r': 0.8792134831460674, 'p': 0.819371727748691, 'f': 0.8482384773910298}, 'rouge-2': {'r': 0.8303130148270181, 'p': 0.7801857585139319, 'f': 0.8044692687478607}, 'rouge-l': {'r': 0.8792134831460674, 'p': 0.819371727748691, 'f': 0.8482384773910298}}, {'rouge-1': {'r': 0.8655616942909761, 'p': 0.793918918918919, 'f': 0.8281938276084381}, 'rouge-2': {'r': 0.8023952095808383, 'p': 0.7635327635327636, 'f': 0.7824817468278971}, 'rouge-l': {'r': 0.8655616942909761, 'p': 0.793918918918919, 'f': 0.8281938276084381}}, {'rouge-1': {'r': 0.8763736263736264, 'p': 0.7799511002444988, 'f': 0.8253557518086654}, 'rouge-2': {'r': 0.7993421052631579, 'p': 0.7431192660550459, 'f': 0.7702060171936479}, 'rouge-l': {'r': 0.8763736263736264, 'p': 0.7799511002444988, 'f': 0.8253557518086654}}, {'rouge-1': {'r': 0.8379888268156425, 'p': 0.7792207792207793, 'f': 0.8075370071196579}, 'rouge-2': {'r': 0.7802385008517888, 'p': 0.7387096774193549, 'f': 0.7589063744569274}, 'rouge-l': {'r': 0.8379888268156425, 'p': 0.7792207792207793, 'f': 0.8075370071196579}}, {'rouge-1': {'r': 0.8671875, 'p': 0.8043478260869565, 'f': 0.8345864611724801}, 'rouge-2': {'r': 0.8304668304668305, 'p': 0.7878787878787878, 'f': 0.8086124351948502}, 'rouge-l': {'r': 0.8671875, 'p': 0.8043478260869565, 'f': 0.8345864611724801}}, {'rouge-1': {'r': 0.8518518518518519, 'p': 0.7976878612716763, 'f': 0.8238805920203164}, 'rouge-2': {'r': 0.7981330221703618, 'p': 0.7533039647577092, 'f': 0.7750708165339196}, 'rouge-l': {'r': 0.8518518518518519, 'p': 0.7976878612716763, 'f': 0.8238805920203164}}]}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "rouge_eval = {}\n",
    "gemini_eval = {}\n",
    "rouge_baseline = {}\n",
    "gemini_baseline = {}\n",
    "\n",
    "\n",
    "for name in [llama_obj['name'], minerva_obj['name']]:\n",
    "    model_scorer = Rouge()\n",
    "    gemini = genai.GenerativeModel(model_name=GEMINI_VERSION)\n",
    "    filename = os.path.join(OUTPUT_PATH, f\"{OUTPUT_PREFIX}-{name}.json\")\n",
    "    gemini_eval[name] = [] \n",
    "    \n",
    "    baseline_set = []\n",
    "    reference_set = []\n",
    "    produced_set = []\n",
    "    \n",
    "    with open(filename, \"r\", encoding='utf-8') as file_desc:\n",
    "        output_log = json.load(file_desc)\n",
    "        file_desc.close()\n",
    "    \n",
    "    for k,v in output_log.items():\n",
    "        produced_set.append(v)\n",
    "        reference_set.append(datasetRaw['cleaned'][f\"{k}\"])\n",
    "        if len(baseline_set) >= int(k)-1:\n",
    "            baseline_set.append(datasetRaw['original_ocr'][f\"{k}\"])\n",
    "    \n",
    "        gemini_input_eval = segment_wrapper(v, datasetRaw['cleaned'][f\"{k}\"])\n",
    "        evaluation_gemini = gemini.generate_content(gemini_input_eval)\n",
    "        gemini_eval[name].append(evaluation_gemini.text)\n",
    "        \n",
    "    rouge_eval[name] = model_scorer.get_scores(produced_set,reference_set)\n",
    "    rouge_baseline[name] = model_scorer.get_scores(baseline_set, reference_set)\n",
    "    \n",
    "print(f\"{'-'*20}\")\n",
    "print(rouge_eval)\n",
    "print(rouge_baseline)\n",
    "print(f\"{'-'*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_formatter(eval_list:list):\n",
    "    res = {}\n",
    "    for n, score in enumerate(eval_list):\n",
    "        res[f\"{n+1}\"] = {\n",
    "            \"rouge-1\": score['rouge-1']['f'],\n",
    "            \"rouge-2\": score['rouge-2']['f'],\n",
    "            \"rouge-l\": score['rouge-l']['f'],\n",
    "        }\n",
    "    return res\n",
    "\n",
    "def gemini_formatter(eval_list:list):\n",
    "    res = {}\n",
    "    for n, sentence in enumerate(eval_list):\n",
    "        scores = sentence.strip().split('\\n')\n",
    "        entry = {}\n",
    "        for s in scores:\n",
    "            s = s.split(\":\")\n",
    "            entry[f\"{s[0]}\"] = int(s[1])/100\n",
    "        res[f\"{n+1}\"] = entry\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_GEMINI = [\"Correttezza\", \"Comprensibilità\", \"Somiglianza\"]\n",
    "\n",
    "llama_gemini = gemini_formatter(gemini_eval[llama_obj['name']])\n",
    "minerva_gemini = gemini_formatter(gemini_eval[minerva_obj['name']])\n",
    "\n",
    "for k in CATEGORIES_GEMINI:\n",
    "    collective_gemini_obj = {}\n",
    "    for n, under_exam in zip([llama_obj['name'], minerva_obj['name']], [llama_gemini, minerva_gemini]):\n",
    "        model_eval_obj = {i:v[k] for i,v in under_exam.items()}\n",
    "        collective_gemini_obj[n] = model_eval_obj\n",
    "    with open(os.path.join(OUTPUT_PATH, f\"{OUTPUT_PREFIX}-gemini_{k}.json\"),\"w\") as file:\n",
    "        json.dump(collective_gemini_obj, file)\n",
    "        file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES_ROUGE = [\"rouge-1\", \"rouge-2\", \"rouge-l\"]\n",
    "\n",
    "llama_rouge = rouge_formatter(rouge_eval[llama_obj['name']])\n",
    "minerva_rouge = rouge_formatter(rouge_eval[minerva_obj['name']])\n",
    "\n",
    "for k in CATEGORIES_ROUGE:\n",
    "    collective_rouge_obj = {}\n",
    "    for n, under_exam in zip([llama_obj['name'], minerva_obj['name']], [llama_rouge, minerva_rouge]):\n",
    "        model_eval_obj = {i:v[k] for i,v in under_exam.items()}\n",
    "        collective_rouge_obj[n] = model_eval_obj\n",
    "    with open(os.path.join(OUTPUT_PATH, f\"{OUTPUT_PREFIX}-{k}_f.json\"),\"w\") as file:\n",
    "        json.dump(collective_rouge_obj, file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prometeus evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2977300f3a4c5cb45a749c9e1276b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "PROMETHEUS_VERSION = \"Unbabel/M-Prometheus-7B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PROMETHEUS_VERSION)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    PROMETHEUS_VERSION,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "outRaw = {}\n",
    "\n",
    "for model_name in [llama_obj['name'], minerva_obj['name']]:\n",
    "    file_name = f\"{OUTPUT_PREFIX}-{model_name}.json\"\n",
    "    if  file_name not in os.listdir(OUTPUT_PATH):\n",
    "        print(f\"ERROR 404 ! File {i} not Found...\")\n",
    "\n",
    "    file_path = os.path.join(OUTPUT_PATH, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        outRaw[model_name.split('.')[0]] = json.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    'Llama': {\n",
    "                \"paragraphs\": [],\n",
    "                \"scores\": []\n",
    "            },\n",
    "    'Minerva': {\n",
    "                \"paragraphs\": [],\n",
    "                \"scores\": []\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama\n",
      "2\n",
      "1: 2\n"
     ]
    }
   ],
   "source": [
    "for model_name in [llama_obj['name'], minerva_obj['name']]:\n",
    "    print(model_name)\n",
    "    for index, paragraph in outRaw[f'{model_name}'].items():\n",
    "        prompt = f\"\"\"\n",
    "            Valuta il testo ripulito da errori OCR generato dal modello rispetto alla corrispondente versione corretta, tenendo conto di correttezza, comprensibilità e somiglianza.\n",
    "            Segui il seguente metro di valutazione:\n",
    "            - 1: Pulizia completamente inaccettabile: Il testo non è stato ripulito e gli errori OCR persistono.\n",
    "            - 2: Gravi errori semantici, omissioni o aggiunte sostanziali al testo originale.\n",
    "            - 3: Pulizia parzialmente sbagliata, l'output ottenuto è superficiale. Contiene ancora errori, ma sono per lo più errori da un punto di vista semantico.\n",
    "            - 4: Buona pulizia. L'outpt del testo è in gran parte corretta e fedele al testo, ma contiene ancora qualche errore minore da un punto di vista semantico. Il testo risulta fluente e comprensibile.\n",
    "            - 5: Pulizia perfetta. Il testo è stato ripulito correttamente da errori OCR.\n",
    "\n",
    "            Versione corretta: {datasetRaw['cleaned'][index]}\n",
    "            Versione {model_name}: {paragraph}\n",
    "\n",
    "            Rispondi solamente con il voto, seguendo il seguente formato per la risposta. \n",
    "            Valutazione: \n",
    "        \"\"\"\n",
    "        \n",
    "        prometheus_output = pipe(prompt)[0][\"generated_text\"]\n",
    "        prometheus_output = prometheus_output.split(\"Valutazione: \")[-1].strip().split(\"\\n\")[0]\n",
    "        scores_dict[model_name]['paragraphs'].append(index)\n",
    "        scores_dict[model_name]['scores'].append(prometheus_output)\n",
    "        print(f\"{index}: {prometheus_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prometheus_formatter(scores_received):\n",
    "    res = {}\n",
    "    for name, content in scores_received.items():\n",
    "        res[name] = []\n",
    "        for index, score in zip(content['paragraphs'], content['scores']):\n",
    "            res[name].append(f\"paragraph {index} : score {score}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "formatted_data = prometheus_formatter(scores_dict)\n",
    "\n",
    "fname = f\"{OUTPUT_PREFIX}-prometheus.json\"\n",
    "with open(os.path.join(OUTPUT_PATH, fname), 'w', encoding='utf-8') as f:\n",
    "    json.dump(formatted_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
