{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLP Homework 2 - OCRed text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Supports float16:\", torch.cuda.is_available())\n",
    "print(\"Supports bfloat16:\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to either set a .env file or put here the keys for gemini and hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '.env' in os.listdir(os.getcwd()):\n",
    "    load_dotenv()\n",
    "    CACHE_SUBPATH = os.environ['CACHE_PATH']\n",
    "    TEXTS_SUBPATH = os.environ['DATA_PATH']\n",
    "    OUTPUT_SUBPATH = os.environ['OUTPUT_PATH']\n",
    "    HF_TOKEN = os.environ['HF_TOKEN']\n",
    "    GEMINI_KEY = os.environ['GEMINI_KEY']\n",
    "    \n",
    "else:\n",
    "    CACHE_SUBPATH = 'cache'\n",
    "    TEXTS_SUBPATH = 'data\\ita'\n",
    "    OUTPUT_SUBPATH = 'cleaned'\n",
    "    HF_TOKEN = ''\n",
    "    GEMINI_KEY = ''\n",
    "\n",
    "os.environ[\"ACCELERATE_USE_TORCH_DEVICE\"] = \"true\"\n",
    "\n",
    "TEXTS_PATH = os.path.join(os.getcwd(), TEXTS_SUBPATH)\n",
    "OUTPUT_PATH =os.path.join(os.getcwd(), OUTPUT_SUBPATH)\n",
    "CACHE_PATH = os.path.join(os.getcwd(), CACHE_SUBPATH)\n",
    "\n",
    "os.makedirs(TEXTS_PATH, exist_ok=True)\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(CACHE_PATH, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = CACHE_PATH\n",
    "\n",
    "FILE_LIST = [\"original_ocr.json\", \"cleaned.json\"]\n",
    "OUTPUT_PREFIX = \"Pizza_Language_&_Mandolino-hw2_ocr\"\n",
    "\n",
    "assert HF_TOKEN != '', \"No key for hugging face\"\n",
    "login(token=HF_TOKEN)\n",
    "assert GEMINI_KEY != '', \"No key for gemini\"\n",
    "genai.configure(api_key=GEMINI_KEY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRaw = {}\n",
    "\n",
    "for name in FILE_LIST:\n",
    "    if name not in os.listdir(TEXTS_PATH):\n",
    "        print(f\"File [{name}] not Found in directory [{TEXTS_PATH}]\")\n",
    "\n",
    "    file_path = os.path.join(TEXTS_PATH, name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        datasetRaw[name.split('.')[0]] = json.load(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_split_paragraphs(text:str)->list[str]:\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "    text = re.sub(r'\\.{2,}', lambda m: f\"<DOTS{len(m.group(0))}>\", text)\n",
    "\n",
    "    fragments = re.split(r'(?<!\\.)\\.(?!\\.)(?=\\s|$)', text)\n",
    "\n",
    "    result = []\n",
    "    for frag in fragments:\n",
    "        frag = frag.strip()\n",
    "        if not frag:\n",
    "            continue\n",
    "        frag = re.sub(r'<DOTS(\\d+)>', lambda m: '.' * int(m.group(1)), frag)\n",
    "        result.append(frag + \". \")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hyphenated_words(text:str)->str:\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasetRaw['original_ocr']['1']\n",
    "sentences = smart_split_paragraphs(sample)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = merge_hyphenated_words(sentences[i])\n",
    "\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    print(f\"{s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(texts: list[str], model_obj: dict)-> list[dict]:\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        \n",
    "        prompt = model_obj['prompt_gen'](texts)\n",
    "        \n",
    "        output = model_obj['generator'](\n",
    "            prompt,\n",
    "            max_new_tokens=len(model_obj['tokenizer'].encode(text)) - 2,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            num_beams=4,\n",
    "            repetition_penalty=1.1,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        cleaned = output.split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "        print(f\"Input:  {text}\")\n",
    "        print(f\"Output: {cleaned}\\n\")\n",
    "        \n",
    "        results.append({\"input\": text, \"output\": cleaned})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_experiment(model_obj:dict) -> None:\n",
    "    print(model_obj['name'])\n",
    "    for chapter, sample in datasetRaw['original_ocr'].items():\n",
    "        print(f\"Chapter {chapter}\")\n",
    "        sentences = smart_split_paragraphs(sample)\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = merge_hyphenated_words(sentences[i])\n",
    "\n",
    "        corrected_data = correct_text(sentences, model_obj)\n",
    "\n",
    "        with open(os.path.join(OUTPUT_PATH,model_obj['name'],f\"Chapter_{chapter}.json\"), \"w+\", encoding=\"utf-8\") as file:\n",
    "            json.dump(corrected_data, file, ensure_ascii=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_output(model_name:str, prefix:str=OUTPUT_PREFIX)->None:\n",
    "    cleaned_txt = {}\n",
    "    dest_path = os.path.join(OUTPUT_PATH, model_name)\n",
    "    for i, file_name in enumerate(os.listdir(dest_path)):\n",
    "        with open(os.path.join(dest_path,file_name), 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "            text = ' '.join(item['output'] for item in data)\n",
    "            cleaned_txt[str(i+1)] = text\n",
    "            file.close()\n",
    "\n",
    "    fname = f\"{prefix}-{model_name}.json\"\n",
    "    with open(os.path.join(OUTPUT_PATH, fname), 'w', encoding='utf-8') as f_out:\n",
    "        json.dump(cleaned_txt, f_out, ensure_ascii=False, indent=2)\n",
    "        f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minerva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINERVA_VERSION = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
    "minerva_tok = AutoTokenizer.from_pretrained(MINERVA_VERSION, trust_remote_code=True)\n",
    "minerva_model = AutoModelForCausalLM.from_pretrained(MINERVA_VERSION, \n",
    "                                            trust_remote_code=True, \n",
    "                                            device_map=device, \n",
    "                                            torch_dtype=torch.float16\n",
    "                                            )\n",
    "\n",
    "minerva_gen = pipeline(\"text-generation\", \n",
    "                        model=minerva_model, \n",
    "                        tokenizer=minerva_tok, \n",
    "                        pad_token_id=minerva_tok.eos_token_id\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minerva_obj = {\n",
    "    \"model\":minerva_model,\n",
    "    \"generator\": minerva_gen,\n",
    "    \"tokenizer\":minerva_tok,\n",
    "    \"name\":\"Minerva\",\n",
    "    \"prompt_gen\": lambda x: (\n",
    "            f\"Sei un generatore di eBooks. \"\n",
    "            f\"Correggi TUTTI gli errori OCR nel testo che ti verrà fornito. \"\n",
    "            f\"Non aggiungere, togliere o cambiare parole. \"\n",
    "            f\"Non modificare la punteggiatura esistente. \"\n",
    "            f\"Non interpretare, riassumere o riscrivere il testo. \"\n",
    "            f\"Mantieni esattamente lo stesso numero di parole e l'ordine delle parole. \"\n",
    "            f\"Il testo corretto deve conservare il significato originale inalterato. \"\n",
    "            f\"La tua unica funzione è la correzione di errori OCR: lettere errate, parole spezzate, ed errori di capitalizzazioni. \"\n",
    "            f\"Rendi maiuscole le iniziali di frase e i nomi propri. Rendi minuscole le parole che non sono nomi propri o inizi di frase ma sono in maiuscolo. \"\n",
    "            f\"Di seguito sono forniti esempi di input OCR e il corrispondente output corretto. Apprendi da questi esempi per svolgere il tuo compito.\\n\"\n",
    "            f\"Esempi di correzione OCR (inclusa capitalizzazione):\\n\"\n",
    "            f\"Input:  'qvesto è un testo. sembra ocr.'\\n\"\n",
    "            f\"Output: 'Questo è un testo. Sembra OCR.'\\n\"\n",
    "            f\"Input:  'l'aquila vola alta. il cieto è blù.'\\n\"\n",
    "            f\"Output: 'L'aquila vola alta. Il cielo è blu.'\\n\"\n",
    "            f\"Input: 'la matita è rota. c'è del tem po.'\\n\"\n",
    "            f\"Output: 'La matita è rotta. C'è del tempo.'\\n\"\n",
    "            f\"Input: 'i1 libro è su1 tavolo. LA STAMPA è chiara.'\\n\"\n",
    "            f\"Output: 'Il libro è sul tavolo. La stampa è chiara.'\\n\"\n",
    "            f\"Input: 'abbiam o un gTande paeco. la vitta è bella.'\\n\"\n",
    "            f\"Output: 'Abbiamo un grande pacco. La vita è bella.'\\n\"\n",
    "            f\"Testo OCR: {x} \"\n",
    "            f\"Testo corretto:\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_experiment(minerva_obj)\n",
    "reformat_output(minerva_obj['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_VERSION = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "llama_tok = AutoTokenizer.from_pretrained(LLAMA_VERSION)\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_VERSION,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "llama_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llama_model,\n",
    "    tokenizer=llama_tok\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_obj = {\n",
    "    \"model\":llama_model,\n",
    "    \"generator\":llama_gen,\n",
    "    \"tokenizer\":llama_tok,\n",
    "    \"name\":\"Llama\",\n",
    "    \"prompt_gen\": lambda x: (\n",
    "            f\"Sei un Grillo Parlante! \"\n",
    "            f\"Correggi TUTTI gli errori OCR nel testo che ti verrà fornito. \"\n",
    "            f\"Non aggiungere, togliere o cambiare parole. \"\n",
    "            f\"Non modificare la punteggiatura esistente. \"\n",
    "            f\"Non interpretare, riassumere o riscrivere il testo. \"\n",
    "            f\"Mantieni esattamente lo stesso numero di parole e l'ordine delle parole. \"\n",
    "            f\"Il testo corretto deve conservare il significato originale inalterato. \"\n",
    "            f\"La tua unica funzione è la correzione di errori OCR: lettere errate, parole spezzate, ed errori di capitalizzazioni. \"\n",
    "            f\"Rendi maiuscole le iniziali di frase e i nomi propri. Rendi minuscole le parole che non sono nomi propri o inizi di frase ma sono in maiuscolo. \"\n",
    "            f\"Di seguito sono forniti esempi di input OCR e il corrispondente output corretto. Apprendi da questi esempi per svolgere il tuo compito.\\n\"\n",
    "            f\"Esempi di correzione OCR (inclusa capitalizzazione):\\n\"\n",
    "            f\"Input: 'qvesto è un testo. sembra ocr.'\\n\"\n",
    "            f\"Output: 'Questo è un testo. Sembra OCR.'\\n\"\n",
    "            f\"Input: 'l'aquila vola alta. il cieto è blù.'\\n\"\n",
    "            f\"Output: 'L'aquila vola alta. Il cielo è blu.'\\n\"\n",
    "            f\"Input: 'la matita è rota. c'è del tem po.'\\n\"\n",
    "            f\"Output: 'La matita è rotta. C'è del tempo.'\\n\"\n",
    "            f\"Input: 'i1 libro è su1 tavolo. LA STAMPA è chiara.'\\n\"\n",
    "            f\"Output: 'Il libro è sul tavolo. La stampa è chiara.'\\n\"\n",
    "            f\"Input: 'abbiam o un gTande paeco. la vitta è bella.'\\n\"\n",
    "            f\"Output: 'Abbiamo un grande pacco. La vita è bella.'\\n\"\n",
    "            f\"Input: 'rn'\\n\"\n",
    "            f\"Output: 'm'\\n\"\n",
    "            f\"Input: 'Ko'\\n\"\n",
    "            f\"Output: 'No'\\n\"\n",
    "            f\"Input: '1ibro'\\n\"  \n",
    "            f\"Output: 'libro'\\n\"\n",
    "            f\"Input: '0cchio'\\n\" \n",
    "            f\"Output: 'occhio'\\n\"\n",
    "            f\"Input: 'cl'\\n\" \n",
    "            f\"Output: 'd'\\n\"\n",
    "            f\"Input: 'e, '\\n\"\n",
    "            f\"Output: 'e '\\n\"\n",
    "            f\"Testo OCR: {x} \"\n",
    "            f\"Testo corretto:\"\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_experiment(llama_obj)\n",
    "reformat_output(llama_obj['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_VERSION = \"gemini-1.5-flash\"\n",
    "\n",
    "def segment_wrapper(segment_ocred, segment_clean):\n",
    "    return f\"\"\"\n",
    "        ### Task: Il primo testo è una versione del secondo estratto con oc-Red, ed è stata processata per ridurre gli errori, valuta con un punteggio tra 1 e 100 tenendo conto di correttezza, comprensibilità e somiglianza\n",
    "        ### Testo da valutare: {segment_ocred}.\n",
    "        ### Testo di confronto: {segment_clean}.\n",
    "        ### Requisiti:\n",
    "            - Scrivi il risultato in formato <criterio>:<punteggio>.\n",
    "            - Non usare altri numeri interi o fai altri commenti\n",
    "        ### Risultato:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_eval = {}\n",
    "gemini_eval = {}\n",
    "rouge_baseline = {}\n",
    "gemini_baseline = {}\n",
    "\n",
    "\n",
    "for name in [llama_obj['name'], minerva_obj['name']]:\n",
    "    model_scorer = Rouge()\n",
    "    gemini = genai.GenerativeModel(model_name=GEMINI_VERSION)\n",
    "    filename = os.path.join(OUTPUT_PATH, f\"{OUTPUT_PREFIX}-{name}.json\")\n",
    "    gemini_eval[name] = [] \n",
    "    \n",
    "    baseline_set = []\n",
    "    reference_set = []\n",
    "    produced_set = []\n",
    "    \n",
    "    with open(filename, \"r\", encoding='utf-8') as file_desc:\n",
    "        output_log = json.load(file_desc)\n",
    "        file_desc.close()\n",
    "    \n",
    "    for k,v in output_log.items():\n",
    "        produced_set.append(v)\n",
    "        reference_set.append(datasetRaw['cleaned'][f\"{k+1}\"])\n",
    "        if len(baseline_set) >= int(k)-1:\n",
    "            baseline_set.append(datasetRaw['original_ocr'][f\"{k+1}\"])\n",
    "    \n",
    "        gemini_input_eval = segment_wrapper(v, datasetRaw['cleaned'][f\"{k+1}\"])\n",
    "        evaluation_gemini = gemini.generate_content(gemini_input_eval)\n",
    "        gemini_eval[name].append(evaluation_gemini.text)\n",
    "        \n",
    "    rouge_eval[name] = model_scorer.get_scores(produced_set,reference_set)\n",
    "    rouge_baseline[name] = model_scorer.get_scores(baseline_set, reference_set)\n",
    "    \n",
    "print(f\"{'-'*20}\")\n",
    "print(rouge_eval)\n",
    "print(rouge_baseline)\n",
    "print(f\"{'-'*20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_formatter(eval_list:list)-> JSON:\n",
    "    res = {}\n",
    "    for n, score in enumerate(eval_list):\n",
    "        res[f\"{n+1}\"] = {\n",
    "            \"rouge-1\": score['rouge-1']['f'],\n",
    "            \"rouge-2\": score['rouge-2']['f'],\n",
    "            \"rouge-l\": score['rouge-l']['f'],\n",
    "        }\n",
    "    return res\n",
    "\n",
    "def gemini_formatter(eval_list:list)-> JSON:\n",
    "    res = {}\n",
    "    for n, sentence in enumerate(eval_list):\n",
    "        scores = sentence.strip().split('\\n')\n",
    "        entry = {}\n",
    "        for s in scores:\n",
    "            s = s.split(\":\")\n",
    "            entry[f\"{s[0]}\"] = int(s[1])/100\n",
    "        res[f\"{n+1}\"] = entry\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [llama_obj['name'], minerva_obj['name']]:\n",
    "    with open(os.path.join(OUTPUT_PATH, f\"{OUTPUT_PREFIX}-{n}_geminiEval.json\"),\"w\") as file:\n",
    "        json.dump(gemini_formatter(gemini_eval[n]))\n",
    "        file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [llama_obj['name'], minerva_obj['name']]:\n",
    "    with open(os.path.join(OUTPUT_PATH, f\"{OUTPUT_PREFIX}-{n}_rouge_f1.json\"),\"w\") as file:\n",
    "        json.dump(rouge_formatter(gemini_eval[n]))\n",
    "        file.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
