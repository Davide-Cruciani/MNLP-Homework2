{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6b0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55077d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "path = os.environ['DATA_PATH']\n",
    "os.environ[\"ACCELERATE_USE_TORCH_DEVICE\"] = \"true\"\n",
    "login(token=os.environ['HF_TOKEN'])\n",
    "cleaned_fn = \"cleaned.json\"\n",
    "ocred_fn = \"original_ocr.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5813f726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168bb4dac5cb4b91adb10eee3972362a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\Desktop\\Università\\MNLP\\Homework2\\cache\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33394bc4f1f4f78b02a8dae02f69a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7446b990e31743b391fa934559c98321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750a468a56814eb589ec78c52625799a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d3ad68dff9493ebc88c9340c8bd8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e85e3b8d4234d10a8c9cebe1f59f82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56012c6482b24fc7a56a5048326d2055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a chat bot that translates the message received from italian to english</s>\n",
      "<|user|>\n",
      "Roma è in centro italia</s>\n",
      "<|assistant|>\n",
      "Roma è in centro italiano.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790bc3cb",
   "metadata": {},
   "source": [
    "We use the tokenizer's chat template to format each message\n",
    " \n",
    "Refer to https://huggingface.co/docs/transformers/main/en/chat_templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a473cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction: Translate the following italian sentence into english: roma è al centro dell' Italia\n",
      "### Response: \n",
      "Roma, the capital of Italy, is located in the center of Italy.\n"
     ]
    }
   ],
   "source": [
    "# prompt = [\n",
    "#     {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"You are a chat bot that translates the message received from italian to english\",\n",
    "#     },\n",
    "#     {\"role\": \"user\", \"content\": \"Roma è in centro italia\"},\n",
    "# ]\n",
    "\n",
    "prompt = \"\"\"\n",
    "### Instruction: Translate the following italian sentence into english: roma è al centro dell' Italia\n",
    "### Response: \n",
    "\"\"\"\n",
    "\n",
    "#prompt = pipe.tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb01384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRaw = {}\n",
    "\n",
    "for i in [ocred_fn, cleaned_fn]:\n",
    "    if i not in os.listdir(path):\n",
    "        print(f\"ERROR 404 ! File {i} not Found...\")\n",
    "\n",
    "    file_path = os.path.join(path, i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        datasetRaw[i.split('.')[0]] = json.load(f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d2ed1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasetRaw['original_ocr']['1']\n",
    "question = f\"\"\"\n",
    "### Instruction: This paragraph in italian contains noise, correct in: {sample}\n",
    "### Response: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f5c7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2170 > 2048). Running this sequence through the model will result in indexing errors\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction: This paragraph in italian contains noise, correct in: I. \n",
      "Como  andò  che  Maestro  Ciliegia,  Megnamc \n",
      "trovò  un  pezzo  di  legno  che  piangeva  e  rideva  come  un  bambino. \n",
      "—  C'era  una  volta.... \n",
      "—  Un  re!  -diranno  subito  i  miei  piccoli  lettori. \n",
      "—  Ko,  ragazzi,  avete  sbagliato.  C'era  una  volta \n",
      "un  pezzo  di  legno. \n",
      "Kon  era  un  legno  di  lusso,  ma  un  semplice \n",
      "pezzo  da  catasta,  di  quelli  che  d' inverno  si  met- \n",
      "tono nelle  stufe  e  nei  caminetti  per  accendere \n",
      "il  fuoco  e  per  riscaldare  le  stanze. \n",
      "\n",
      "Non  so  come  andasse,  ma  il  fatto  gli  è  che  un \n",
      "bel  giorno' questo  pezzo  di  legno  capitò  nella  bot- \n",
      "tega di  un  vecchio  falegname,  il  quale  aveva \n",
      "nome  mastr' Antonio,  se  non  che  tutti  lo  chia- \n",
      "mavano maestro  Ciliegia,  per  via  della  punta  del \n",
      "\n",
      "sentì  nna  vocina  sottile  sottile. \n",
      "\n",
      "SUO  naso,  che  era  sempre  lustra  e  paonazza,  come \n",
      "una  ciliegia  matura. \n",
      "Appena  maestro  Ciliegia  ebbe  visto  quel  pezzo \n",
      "di  legno,  si  rallegrò  tutto;  e  dandosi  una  fregatina \n",
      "di  mani  per  la  contentezza,  borbottò  a  mezza  voce: \n",
      "—  Questo  legno  è  capitato  a  tempo;  voglio \n",
      "servirmene  per  fare  una  gamba  di  tavolino.  — \n",
      "\n",
      "Detto  fatto,  prese  subito  l'ascia  arrotata  per \n",
      "cominciare  a  levargli  la  scorza  e  a  digrossarlo; \n",
      "ma  quando  fu  lì  per  lasciare  andare  la  prima \n",
      "asciata,  rimase  col  braccio  sospeso  in  aria,  per- \n",
      "chè sentì  una  vocina  sottile  sottile,  che  disse \n",
      "raccomandandosi: \n",
      "—  Non  mi  picchiar  tanto  forte!  — \n",
      "Figuratevi  come  rimase  quel  buon  vecchio  di \n",
      "maestro  Ciliegia! \n",
      "Girò  gli  occhi  smarriti  intorno  alla  stanza  per \n",
      "vedere  di  dove  mai  poteva  essere  uscita  quella  vo- \n",
      "cina, e  non  vide  nessuno  !  Guardò  sotto  il  banco, \n",
      "e  nessuno  :  guardò  dentro  un  armadio  che  stava \n",
      "sempre  chiuso,  e  nessuno;  guardò  nel  corbèllo \n",
      "dei  trucioli  e  della  segatura,  e  nessuno;  aprì  l'uscio \n",
      "di  bottega  per  dare  un'occhiata  anche  sulla  strada, \n",
      "e  nessuno.  O  dunque?... \n",
      "—  Ho  capito;  —  disse  allora  ridendo  e  gTat- \n",
      "tandosi  la  parrucca  —  si  vede  che  quella  vocina \n",
      "me  la  son  figurata  io.  Eimettiamoci  a  lavo- \n",
      "rare. — \n",
      "E  ripresa  l' ascia  in  mano,  tirò  giù  un  solennis- \n",
      "simo  colpo  sul  pezzo  di  legno. \n",
      "—  Ohi!  tu  m'hai  fatto  male!  —  gridò  ramma- \n",
      "ricandosi la  solita  vocina. \n",
      "Questa  volta  maestro  Ciliegia  restò  di  stucco, \n",
      "cogli  occhi  fuori  del  capo  per  la  paura,  colla  bocca \n",
      "\n",
      "spalancati  e  colla  lingna  giù  ciondoloni  fino  al \n",
      "mento,  come  nn  mascherone  da  fontana. \n",
      "Appena  riebbe  l'uso  della  parola,  cominciò  a \n",
      "dire  tremando  e  balbettando  dallo  spavento: \n",
      "—  Ma  di  dove  sarà  uscita  questa  vocina  che \n",
      "ha  detto  ohi  ?...  Eppure  qui  non  e'  è  anima  viva. \n",
      "Ohe  sia  per  caso  questo  pezzo  di  legno  che  abbia \n",
      "imparato  a  piangere  e  a  lamentarsi  come  un  bam- \n",
      "bino? Io  non  lo  posso  credere.  Questo  legno  eccolo \n",
      "qui;  è  un  pezzo  di  legno  da  caminetto,  come  tutti \n",
      "gli  altri,  e  a  buttarlo  sul  fuoco,  c'è  da  far  bollire \n",
      "una  pentola  di  fagioli....  O  dunque?  Ohe  ci  sia  na- \n",
      "scosto dentro  qualcuno!  Se  c'è  nascosto  qualcuno, \n",
      "tanto  peggio  per  lui.  Ora  l'accomodo  io!  — \n",
      "E  così  dicendo,  agguantò  con  tutt'  e  due  le \n",
      "mani  quel  povero  pezzo  di  legno,  e  si  pose  a \n",
      "sbatacchiarlo  senza  carità  contro  le  pareti  della \n",
      "stanza. \n",
      "Poi  si  messe  in  ascolto,  per  sentire  se  c'era \n",
      "qualche  vocina  che  si  lamentasse.  Aspettò  due \n",
      "minuti,  e  nulla;  cinque  minuti,  e  nulla;  dieci  mi- \n",
      "nuti, e  nulla! \n",
      "— -  Ho  capito  —  disse  allora  sforzandosi  di  ri- \n",
      "dere e  arruffandosi  la  parrucca — si  vede  che  quella \n",
      "vocina  che  ha  detto  olii,  me  la  son  figurata  io! \n",
      "Eimettiamoci  a  lavorare.  — \n",
      "E  perchè  gli  era  entrato  addosso  una  gran \n",
      "\n",
      "\n",
      "paura,  si  provò  a  canterellare  per  farsi  un  po'  di \n",
      "coraggio. \n",
      "Intanto,  i^osata  da  una  parte  l'ascia,  prese  in \n",
      "mano  la  pialla,  per  piallare  e  tirare  a  pulimento \n",
      "il  pezzo  di  legno  ;  ma  nel  mentre  che  lo  piallava \n",
      "in  su  e  in  giù,  sentì  la  solita  vocina  che  gli  disse \n",
      "ridendo  : \n",
      "—  Smetti!  tu  mi  fai  il  pizzicorino  sul  corpo!  — \n",
      "Questa  volta  il  povero  maestro  Ciliegia  cadde \n",
      "giù  come  fulminato.  Quando  riaprì  gli  occhi,  si \n",
      "trovò  seduto  per  terra. \n",
      "Il  suo  viso  pareva  trasfigurito,  e  perfino  la  punta \n",
      "del  naso,  di  paonazza  come  era  quasi  sempre,  gli \n",
      "era  diventata  turchina  dalla  gran  x>ai^i'?^' \n",
      "\n",
      "pyyyyy^y^'^wggfy; \n",
      "\n",
      "ti;;::;i;r\n",
      "### Response: \n",
      "Asperse  a  works  with  a  pialla,  prese  in  mani,  for,  and Alettersay \n",
      "he  pialy  luvian  piala  maestro  ciliegia  ma'y^y'pyyyyy^\n",
      "\n",
      "^;x::; \n",
      "^piyt'^'::ai  ; \n",
      "^x^y' \n",
      "pye^y^^y^ai^p'y^'pi \n",
      "y'e;  y'pivey|nonas^oiiy;y;o|y|y'i|oy \n",
      "\n",
      "^yiiyi|yi 1, do|ai&xosto\n",
      "\n",
      "\n",
      "yoti^y  oypy \n",
      " \n",
      "<|ii|&y \n",
      "y|o|o  oyy|oyi|yiyy|y.y in; y. \n",
      "|y&yøy,oyy 3|y;o;i.\n",
      "—i||y ivi&y \n",
      "y| \n",
      "y ø|y&i&yii?|o,\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(question, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
