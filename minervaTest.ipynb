{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186320ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "path = os.environ['DATA_PATH']\n",
    "os.environ[\"ACCELERATE_USE_TORCH_DEVICE\"] = \"true\"\n",
    "login(token=os.environ['HF_TOKEN'])\n",
    "nltk.download('punkt')\n",
    "cleaned_fn = \"cleaned.json\"\n",
    "ocred_fn = \"original_ocr.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28065002",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Supports float16:\", torch.cuda.is_available())\n",
    "print(\"Supports bfloat16:\", torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "minervaId = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "# minervaId = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(minervaId)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(minervaId, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c81dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRaw = {}\n",
    "\n",
    "for i in [ocred_fn, cleaned_fn]:\n",
    "    if i not in os.listdir(path):\n",
    "        print(f\"ERROR 404 ! File {i} not Found...\")\n",
    "\n",
    "    file_path = os.path.join(path, i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        datasetRaw[i.split('.')[0]] = json.load(f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ed24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasetRaw['original_ocr']['1']\n",
    "\n",
    "sentences = sent_tokenize(sample)\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "adaptor = lambda x: f\"\"\"\n",
    "Sei un esperto di lingua italiana.\n",
    "Questo testo contiene errori dovuti al fatto che è stato estratto da un immagine.\n",
    "Correggi il testo mantenendo il più possibile le parole originali.\n",
    "Non inserire elenchi o numerazioni.\n",
    "Non aggiungere commenti o testo extra dopo la correzione.\n",
    "Testo originale: {x}\n",
    "Testo corretto:\n",
    "\"\"\"\n",
    "prompts = [adaptor(sentence) for sentence in sentences]\n",
    "input_tensor = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_tensor[\"input_ids\"])\n",
    "print(\"Max token ID:\", input_tensor[\"input_ids\"].max().item())\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba03bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor = model.generate(\n",
    "    input_tensor[\"input_ids\"],\n",
    "    attention_mask=input_tensor[\"attention_mask\"],\n",
    "    max_new_tokens=512,\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=False,\n",
    "    num_beams=3,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a170491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_extracted = [tokenizer.decode(res, skip_special_tokens=True) for res in output_tensor]\n",
    "for i, out in enumerate(text_extracted, 1):\n",
    "    print(f\"Output {i}:\\n{out}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149aa0a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_words(sentence):\n",
    "    sentence = sentence.split()\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dba791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Setup modello ===\n",
    "model_name = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='cuda', torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# === 2. Token matching check ===\n",
    "# def token_match_ratio(input_text, output_text):\n",
    "#     in_words = input_text.lower().split()\n",
    "#     out_words = output_text.lower().split()\n",
    "#     matches = sum(1 for w in out_words if w in in_words)\n",
    "#     return matches / max(len(in_words), len(out_words))\n",
    "import difflib\n",
    "\n",
    "def token_match_ratio(input_text, output_text):\n",
    "    in_words = input_text.lower().split()\n",
    "    out_words = output_text.lower().split()\n",
    "\n",
    "    matches = sum(1 for w in out_words if w in in_words)\n",
    "    base_ratio = matches / max(len(in_words), len(out_words))\n",
    "\n",
    "    # Verifica che ci sia almeno una parola diversa\n",
    "    is_modified = any(i != o for i, o in zip(in_words, out_words)) or len(in_words) != len(out_words)\n",
    "\n",
    "    return base_ratio, is_modified\n",
    "\n",
    "\n",
    "# def regenerate_until_fidelity(text, generator, attempts=5, fidelity_threshold=0.7):\n",
    "#     prompt = (\n",
    "#         f\"Correggi solo gli errori OCR senza cambiare la struttura. \"\n",
    "#         f\"Testo OCR: {text}\\nTesto corretto:\"\n",
    "#     )\n",
    "# \n",
    "#     # Genera più output in parallelo\n",
    "#     outputs = generator(\n",
    "#         prompt,\n",
    "#         max_new_tokens=len(text.split()) + 10,\n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         temperature=0.7,\n",
    "#         num_return_sequences=attempts\n",
    "#     )\n",
    "# \n",
    "#     best_result = None\n",
    "#     best_ratio = 0.0\n",
    "# \n",
    "#     for o in outputs:\n",
    "#         generated = o[\"generated_text\"].split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "#         ratio = token_match_ratio(text, generated)\n",
    "# \n",
    "#         if ratio > best_ratio:\n",
    "#             best_result = generated\n",
    "#             best_ratio = ratio\n",
    "# \n",
    "#         if ratio >= fidelity_threshold:\n",
    "#             break  # early stop se già buono\n",
    "# \n",
    "#     return {\n",
    "#         \"input\": text,\n",
    "#         \"output\": best_result,\n",
    "#         \"fidelity\": round(best_ratio, 2),\n",
    "#         \"ok\": best_ratio >= fidelity_threshold\n",
    "#     }\n",
    "\n",
    "def regenerate_until_fidelity(text, generator, attempts=5, fidelity_threshold=0.7):\n",
    "    prompt = (\n",
    "        f\"Correggi solo gli errori OCR senza cambiare la struttura. \"\n",
    "        f\"Testo OCR: {text}\\nTesto corretto:\"\n",
    "    )\n",
    "\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=len(text.split()) + 10,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    best_ratio = 0.0\n",
    "\n",
    "    for o in outputs:\n",
    "        generated = o[\"generated_text\"].split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "        ratio, modified = token_match_ratio(text, generated)\n",
    "\n",
    "        # Deve essere sia sufficientemente simile sia modificato\n",
    "        if ratio > best_ratio and modified:\n",
    "            best_result = generated\n",
    "            best_ratio = ratio\n",
    "\n",
    "        if ratio >= fidelity_threshold and modified:\n",
    "            break  # early stop su buon output\n",
    "\n",
    "    return {\n",
    "        \"input\": text,\n",
    "        \"output\": best_result if best_result else text,\n",
    "        \"fidelity\": round(best_ratio, 2),\n",
    "        \"ok\": best_ratio >= fidelity_threshold and best_result != text\n",
    "    }\n",
    "\n",
    "\n",
    "# === 3. Funzione principale con controllo fedeltà\n",
    "def correct_with_minerva_and_check(texts, fidelity_threshold=0.6):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        #prompt = (\n",
    "        #    f\"Correggi solo gli errori OCR senza cambiare la struttura. \"\n",
    "        #    f\"Testo OCR: {text}\\nTesto corretto:\"\n",
    "        #)\n",
    "        #output = generator(prompt, max_new_tokens=len(text.split()) + 10, do_sample=False)[0][\"generated_text\"]\n",
    "        #corrected = output.split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "        #ratio = token_match_ratio(text, corrected)\n",
    "\n",
    "        #results.append({\n",
    "        #    \"input\": text,\n",
    "        #    \"output\": corrected,\n",
    "        #    \"fidelity\": round(ratio, 2),\n",
    "        #    \"ok\": ratio >= fidelity_threshold\n",
    "        #})\n",
    "        result = regenerate_until_fidelity(text, generator, attempts=5)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# === 4. Esempio\n",
    "ocr_text = \"Qvando il sole sorge, le ombrre svannos noel silenzio del matino.\"\n",
    "results = correct_with_minerva_and_check([ocr_text])\n",
    "\n",
    "# === 5. Stampa con feedback\n",
    "for r in results:\n",
    "    print(\"Input:  \", r[\"input\"])\n",
    "    print(\"Output: \", r[\"output\"])\n",
    "    print(\"Fidelity ratio:\", r[\"fidelity\"])\n",
    "    print(\"🟢 Accettato\" if r[\"ok\"] else \"🔴 Da rigenerare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0149aa0a",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_words(sentence):\n",
    "    sentence = sentence.split()\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# === 1. Carica modello Minerva ===\n",
    "model_name = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='cuda', torch_dtype=torch.float16)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# === 3. Funzione corretta per OCR fixing ===\n",
    "def correct_with_minerva(texts):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        prompt = (\n",
    "            f\"Correggi gli errori OCR senza cambiare la struttura del testo. \"\n",
    "            f\"Non riscrivere. Correggi solo lettere errate. \"\n",
    "            f\"Testo OCR: {text} \"\n",
    "            f\"Testo corretto:\"\n",
    "        )\n",
    "        output = generator(prompt, max_new_tokens=number_words(prompt) + number_words(text), do_sample=False)[0][\"generated_text\"]\n",
    "        \n",
    "        # Estrai solo il contenuto generato dopo \"Testo corretto:\"\n",
    "        cleaned = output.split(\"Testo corretto:\")[-1].strip()\n",
    "        \n",
    "        # Stop eventuale se modello prolunga troppo\n",
    "        if \"\\n\" in cleaned:\n",
    "            cleaned = cleaned.split(\"\\n\")[0]\n",
    "        \n",
    "        results.append({\"input\": text, \"output\": cleaned})\n",
    "    return results\n",
    "\n",
    "# === 4. Test ===\n",
    "ocr_text = \"Qvando il sole sorge, le ombrre svannos noel silenzio del matino.\"\n",
    "corrected_data = correct_with_minerva([ocr_text])\n",
    "\n",
    "# === 5. Stampa risultato ===\n",
    "for clean in corrected_data:\n",
    "    print(f\"Input:  {clean['input']}\")\n",
    "    print(f\"Output: {clean['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bb912",
   "metadata": {},
   "source": [
    "Non so perchè ma su un'altro notebook, stesse settings e prompt stampava:\n",
    "\n",
    "===========================================================================\n",
    "\n",
    "Device set to use cuda  \n",
    "Input:  Qvando il sole sorge, le ombrre svannos noel silenzio del matino.  \n",
    "Output: Il sole sorge, le ombre svaniscono.\n",
    "\n",
    "==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dba791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Setup modello ===\n",
    "model_name = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='cuda', torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# === 2. Token matching check ===\n",
    "# def token_match_ratio(input_text, output_text):\n",
    "#     in_words = input_text.lower().split()\n",
    "#     out_words = output_text.lower().split()\n",
    "#     matches = sum(1 for w in out_words if w in in_words)\n",
    "#     return matches / max(len(in_words), len(out_words))\n",
    "import difflib\n",
    "\n",
    "def token_match_ratio(input_text, output_text):\n",
    "    in_words = input_text.lower().split()\n",
    "    out_words = output_text.lower().split()\n",
    "\n",
    "    matches = sum(1 for w in out_words if w in in_words)\n",
    "    base_ratio = matches / max(len(in_words), len(out_words))\n",
    "\n",
    "    # Verifica che ci sia almeno una parola diversa\n",
    "    is_modified = any(i != o for i, o in zip(in_words, out_words)) or len(in_words) != len(out_words)\n",
    "\n",
    "    return base_ratio, is_modified\n",
    "\n",
    "\n",
    "# def regenerate_until_fidelity(text, generator, attempts=5, fidelity_threshold=0.7):\n",
    "#     prompt = (\n",
    "#         f\"Correggi solo gli errori OCR senza cambiare la struttura. \"\n",
    "#         f\"Testo OCR: {text}\\nTesto corretto:\"\n",
    "#     )\n",
    "# \n",
    "#     # Genera più output in parallelo\n",
    "#     outputs = generator(\n",
    "#         prompt,\n",
    "#         max_new_tokens=len(text.split()) + 10,\n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         temperature=0.7,\n",
    "#         num_return_sequences=attempts\n",
    "#     )\n",
    "# \n",
    "#     best_result = None\n",
    "#     best_ratio = 0.0\n",
    "# \n",
    "#     for o in outputs:\n",
    "#         generated = o[\"generated_text\"].split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "#         ratio = token_match_ratio(text, generated)\n",
    "# \n",
    "#         if ratio > best_ratio:\n",
    "#             best_result = generated\n",
    "#             best_ratio = ratio\n",
    "# \n",
    "#         if ratio >= fidelity_threshold:\n",
    "#             break  # early stop se già buono\n",
    "# \n",
    "#     return {\n",
    "#         \"input\": text,\n",
    "#         \"output\": best_result,\n",
    "#         \"fidelity\": round(best_ratio, 2),\n",
    "#         \"ok\": best_ratio >= fidelity_threshold\n",
    "#     }\n",
    "\n",
    "def regenerate_until_fidelity(text, generator, attempts=5, fidelity_threshold=0.7):\n",
    "    prompt = (\n",
    "        f\"Correggi solo gli errori OCR senza cambiare la struttura. \"\n",
    "        f\"Testo OCR: {text}\\nTesto corretto:\"\n",
    "    )\n",
    "\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=len(text.split()) + 10,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    best_result = None\n",
    "    best_ratio = 0.0\n",
    "\n",
    "    for o in outputs:\n",
    "        generated = o[\"generated_text\"].split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "        ratio, modified = token_match_ratio(text, generated)\n",
    "\n",
    "        # Deve essere sia sufficientemente simile sia modificato\n",
    "        if ratio > best_ratio and modified:\n",
    "            best_result = generated\n",
    "            best_ratio = ratio\n",
    "\n",
    "        if ratio >= fidelity_threshold and modified:\n",
    "            break  # early stop su buon output\n",
    "\n",
    "    return {\n",
    "        \"input\": text,\n",
    "        \"output\": best_result if best_result else text,\n",
    "        \"fidelity\": round(best_ratio, 2),\n",
    "        \"ok\": best_ratio >= fidelity_threshold and best_result != text\n",
    "    }\n",
    "\n",
    "\n",
    "# === 3. Funzione principale con controllo fedeltà\n",
    "def correct_with_minerva_and_check(texts, fidelity_threshold=0.6):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        #prompt = (\n",
    "        #    f\"Correggi solo gli errori OCR senza cambiare la struttura. \"\n",
    "        #    f\"Testo OCR: {text}\\nTesto corretto:\"\n",
    "        #)\n",
    "        #output = generator(prompt, max_new_tokens=len(text.split()) + 10, do_sample=False)[0][\"generated_text\"]\n",
    "        #corrected = output.split(\"Testo corretto:\")[-1].strip().split(\"\\n\")[0]\n",
    "\n",
    "        #ratio = token_match_ratio(text, corrected)\n",
    "\n",
    "        #results.append({\n",
    "        #    \"input\": text,\n",
    "        #    \"output\": corrected,\n",
    "        #    \"fidelity\": round(ratio, 2),\n",
    "        #    \"ok\": ratio >= fidelity_threshold\n",
    "        #})\n",
    "        result = regenerate_until_fidelity(text, generator, attempts=5)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# === 4. Esempio\n",
    "ocr_text = \"Qvando il sole sorge, le ombrre svannos noel silenzio del matino.\"\n",
    "results = correct_with_minerva_and_check([ocr_text])\n",
    "\n",
    "# === 5. Stampa con feedback\n",
    "for r in results:\n",
    "    print(\"Input:  \", r[\"input\"])\n",
    "    print(\"Output: \", r[\"output\"])\n",
    "    print(\"Fidelity ratio:\", r[\"fidelity\"])\n",
    "    print(\"🟢 Accettato\" if r[\"ok\"] else \"🔴 Da rigenerare\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
