{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4c38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prometheus-eval accelerate bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20640199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# load_dotenv()\n",
    "# path = os.environ['DATA_PATH']\n",
    "# os.environ[\"ACCELERATE_USE_TORCH_DEVICE\"] = \"true\"\n",
    "path = os.path.join(os.getcwd(), \"Desktop\", \"AI_R\", \"Formazione\", \"data\", \"ita\")\n",
    "hf_token_path = os.path.join(os.getcwd(), \"Desktop\", \"AI_R\", \"Formazione\", \"HF_TOKEN.txt\")\n",
    "with open(hf_token_path, \"r\") as f:\n",
    "    hf_token = f.read().strip()\n",
    "\n",
    "login(token=hf_token)\n",
    "cleaned_fn = \"cleaned.json\"\n",
    "ocred_fn = \"original_ocr.json\"\n",
    "\n",
    "llama_out_fn = \"Pizza_Language_&_Mandolino-hw2_ocr-llama.json\"\n",
    "minerva_out_fn = \"Pizza_Language_&_Mandolino-hw2_ocr-minerva.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2d575314",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRaw = {}\n",
    "\n",
    "for i in [ocred_fn, cleaned_fn]:\n",
    "    if i not in os.listdir(path):\n",
    "        print(f\"ERROR 404 ! File {i} not Found...\")\n",
    "\n",
    "    file_path = os.path.join(path, i)\n",
    "    with open(file_path, 'r') as f:\n",
    "        datasetRaw[i.split('.')[0]] = json.load(f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ddbb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outRaw = {}\n",
    "\n",
    "for i in [llama_out_fn, minerva_out_fn]:\n",
    "    if i not in os.listdir(os.path.join(os.getcwd(), \"Desktop\", \"AI_R\", \"Formazione\")):\n",
    "        print(f\"ERROR 404 ! File {i} not Found...\")\n",
    "\n",
    "    file_path = os.path.join(os.getcwd(), \"Desktop\", \"AI_R\", \"Formazione\", i)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        outRaw[i.split('.')[0]] = json.load(f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21d585da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. Come andò che Maestro Ciliegia, Megnamc trovò un pezzo di legno che piangeva e rideva come un bambino. — C'era una volta.... — Un re! -diranno subito i miei piccoli lettori. — No, ragazzi, avete sbagliato. C'era una volta un pezzo di legno. Con era un legno di lusso, ma un semplice pezzo da catasta, di quelli che d'inverno si mettono nelle stufe e nei caminetti per accendere il fuoco e per riscaldare le stanze. Non so come andasse, ma il fatto gli è che un bel giorno' questo pezzo di legno capitò nella bottega di un vecchio falegname, il quale aveva nome mastr' Antonio, se non che tutti lo chiamavano maestro Ciliegia, per via della punta del sentì nna vocina sottile sottile. SUO naso, che era sempre lustra e paonazza, come una ciliegia matura. Appena maestro Ciliegia ebbe visto quel pezzo di legno, si rallegrò tutto; e dandosi una fregatina di mani per la contentezza, borbottò a mezza voce: — Questo legno è capitato a tempo; voglio servirmene per fare una gamba di tavolino. — Detto fatto, prese subito l'ascia arrotata per cominciare a levargli la scorza e a digrossarlo; ma quando fu lì per lasciare andare la prima asciata, rimase col braccio sospeso in aria, perchè sentì una vocina sottile sottile, che disse raccomandandosi: — Non mi picchiar tanto forte! — Figuratevi come rimase quel buon vecchio di maestro Ciliegia! Girò gli occhi smarriti intorno alla stanza per vedere di dove mai poteva essere uscita quella vocina, e non vide nessuno! Guardò sotto il banco, e nessuno : guardò dentro un armadio che stava sempre chiuso, e nessuno; guardò nel corbèllo dei trucioli e della segatura, e nessuno; aprì l'uscio di bottega per dare un'occhiata anche sulla strada, e nessuno. O dunque?... — Ho capito; — disse allora ridendo e gattandosi la parrucca — si vede che quella vocina me la son figurata io. Eimettiamoci a lavorare. — E ripresa l'ascia in mano, tirò giù un solennissimo colpo sul pezzo di legno. — Ohi! tu m'hai fatto male! — gridò rammaricandosi la solita vocina. Questa volta maestro Ciliegia restò di stucco, cogli occhi fuori del capo per la paura, colla bocca spalancati e colla lingua giù ciondoloni fino al mento, come non mascherone da fontana. Appena riebbe l'uso della parola, cominciò a dire tremando e balbettando dallo spavento: — Ma di dove sarà uscita questa vocina che ha detto ohi?... Eppure qui non è anima viva. Ohe sia per caso questo pezzo di legno che abbia imparato a piangere e a lamentarsi come un bambino? Io non lo posso credere. Questo legno eccolo qui; è un pezzo di legno da caminetto, come tutti gli altri, e a buttarlo sul fuoco, c'è da far bollire una pentola di fagioli.... O dunque? Ohe ci sia nascosto dentro qualcuno! Se c'è nascosto qualcuno, tanto peggio per lui. Ora l'accomodo io! — E così dicendo, agguantò con tutt' e due le mani quel povero pezzo di legno, e si pose a sbatacchiarlo senza carità contro le pareti della stanza. Poi si mise in ascolto, per sentire se c'era qualche voce che si lamentasse. Aspettò due minuti, e nulla; cinque minuti, e nulla; dieci minuti, e nulla! — - Ho capito — disse allora sforzandosi di ridere e arruffandosi la parrucca — si vede che quella vocina che ha detto \"olio\", me la son figurata io! Eimettiamoci a lavorare. — Perchè gli era entrato addosso una gran paura, si provò a canterellare per farsi un po' di coraggio. Intanto, i^osata da una parte l'ascia, prese in mano la pialla, per piallare e tirare a pulimento il pezzo di legno ; ma nel mentre che lo piallava in su e in giù, sentì la solita vocina che gli disse ridendo : — Smetti! tu mi fai il pizzicorino sul corpo! — Questa volta il povero maestro Ciliegia cadde giù come fulminato. Quando riaprì gli occhi, si trovò seduto per terra. Il suo viso pareva trasfigurato, e perfino la punta del naso, di paonazza come era quasi sempre, gli era diventata turchina dalla gran febbre.\n"
     ]
    }
   ],
   "source": [
    "print(outRaw['Pizza_Language_&_Mandolino-hw2_ocr-llama']['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3d68fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c83bb8f5064cd498eec0aeabcc3107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Unbabel/M-Prometheus-7B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "889876b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "88bd8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    'llama': {\n",
    "                \"paragraphs\": [],\n",
    "                \"scores\": []\n",
    "            },\n",
    "    'minerva': {\n",
    "                \"paragraphs\": [],\n",
    "                \"scores\": []\n",
    "            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "eed76efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama\n",
      "1: 2\n",
      "2: 2\n",
      "3: 1\n",
      "4: 2\n",
      "5: Voto: 3\n",
      "6: 2\n",
      "7: 3\n",
      "minerva\n",
      "1: 2\n",
      "2: 2\n",
      "3: 2\n",
      "4: 2\n",
      "5: 2\n",
      "6: Voto: 2\n",
      "7: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name in ['llama', 'minerva']:\n",
    "    print(model_name)\n",
    "    for paragraph in range(1, 8):\n",
    "        prompt = f\"\"\"\n",
    "            Valuta il testo ripulito da errori OCR generato dal modello rispetto alla corrispondente versione corretta, tenendo conto di correttezza, comprensibilità e somiglianza.\n",
    "            Segui il seguente metro di valutazione:\n",
    "            - 1: Pulizia completamente inaccettabile: Il testo non è stato ripulito e gli errori OCR persistono.\n",
    "            - 2: Gravi errori semantici, omissioni o aggiunte sostanziali al testo originale.\n",
    "            - 3: Pulizia parzialmente sbagliata, l'output ottenuto è superficiale. Contiene ancora errori, ma sono per lo più errori da un punto di vista semantico.\n",
    "            - 4: Buona pulizia. L'outpt del testo è in gran parte corretta e fedele al testo, ma contiene ancora qualche errore minore da un punto di vista semantico. Il testo risulta fluente e comprensibile.\n",
    "            - 5: Pulizia perfetta. Il testo è stato ripulito correttamente da errori OCR.\n",
    "\n",
    "            Versione corretta: {datasetRaw['cleaned'][str(paragraph)]}\n",
    "            Versione {model_name}: {outRaw[f'Pizza_Language_&_Mandolino-hw2_ocr-{model_name}'][str(paragraph)]}\n",
    "\n",
    "            Rispondi solamente con il voto, seguendo il seguente formato per la risposta. \n",
    "            Valutazione: \n",
    "        \"\"\"\n",
    "        \n",
    "        result = pipe(prompt)[0][\"generated_text\"]\n",
    "        result = result.split(\"Valutazione: \")[-1].strip().split(\"\\n\")[0]\n",
    "        scores_dict[model_name]['paragraphs'].append(str(paragraph))\n",
    "        scores_dict[model_name]['scores'].append(result)\n",
    "        print(f\"{paragraph}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "369c29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llama': {'paragraphs': ['1', '2', '3', '4', '5', '6', '7'], 'scores': ['2', '2', '1', '2', '3', '2', '3']}, 'minerva': {'paragraphs': ['1', '2', '3', '4', '5', '6', '7'], 'scores': ['2', '2', '2', '2', '2', '2', '2']}}\n"
     ]
    }
   ],
   "source": [
    "print(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9beb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = {\n",
    "    model: [\n",
    "        f\"paragraph {p} : score {s}\"\n",
    "        for p, s in zip(content['paragraphs'], content['scores'])\n",
    "    ]\n",
    "    for model, content in scores_dict.items()\n",
    "}\n",
    "\n",
    "fname = \"Pizza_Language_&_Mandolino-hw2_ocr-prometheus.json\"\n",
    "with open(os.path.join(os.getcwd(), \"Desktop\", \"AI_R\", \"Formazione\", fname), 'w', encoding='utf-8') as f:\n",
    "    json.dump(formatted_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
